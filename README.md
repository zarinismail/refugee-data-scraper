# UNHCR Refugee Data Scraper
For this web scraping project, I scraped [refugee applications data from the UN Refugee Agency](https://www.unhcr.org/refugee-statistics/data-summaries?data_summaries%5Bregion%5D=&data_summaries%5Bcountry%5D=&data_summaries%5Bview%5D=asylum_applications_decisions&data_summaries%5Byear%5D=2024&data_summaries%5BpopType%5D=FDP&data_summaries%5B_mode%5D=global&data_summaries%5B_token%5D=ef68d3c6beb80e06376d53.FaXLcL5eMDtwhpCEae43N_1JYBTKX1wLC5STmU3ZE6Y.ccOvL-QIV1IJ8vy8BbZlGpN5EHCAHAZlU_Gh_HyLTNF087pJ7gRJeRjl_w&data_summaries%5Bsubmit%5D=) (also known as the UNHCR). The site's data finder provides several charts and summary pages for its included countries but no accessible CSV file with the raw data.

## Objective
This scraper collects four data points per country/entity (229 are included on the site): new refugee applications, refugee recognitions or protections, refugee rejections, and pending applications. The data points are then written into a comma-separated values (CSV) file. All data are updated as of 2024.

## Steps
1. I created a `get_urls()` function that would retrieve the option values of countries from a dropdown menu on my main page. These option values had to be appended somewhere within the main URL to create country URLs, as a unique option value is used in every country URL to specify which country is selected from the dropdown menu. These URLs are compiled into a list that is returned through the `get_urls()` function.
2. I created a `get_info()` function that retrieves the country/entity name and the four data points from the country's summary page. These five items are put into a list that is returned through this function.
3. I created a final `write_csv()` function that opens a new CSV file writes into it the list of data points returned through the `get_info()` function into a row for every URL in a list of URLs returned through the `get_urls()` function.

## Unexpected Problem, Solution
While finalizing the scraper, I had trouble getting my contents to write into a CSV file. Knowing I had several pages to scrape, I started by testing the scraper with 10 pages (using `slice()` on my list of URLs). The program successfully created a CSV file with 11 rows (my column heading row and 10 rows of data).

Once I thought my scraper was ready and ran it using all 229 pages, a read-only CSV file with no data in it was created immediately. At first, I thought this was because the CSV file was still being written, but after returning to my computer an hour later, the file was still empty. I realized that I restarted Jupyter Notebook before I ran my code, and I only imported my libraries and ran the `write_csv()` function. I forgot to run my `get_urls()` and `get_info()` functions, but once I did and ran my `write_csv()` function again, a CSV file with five columns and 230 rows was written after about 30 minutes. My program had to pause in between making a request and making soup for every page, so the scraping process took some time.